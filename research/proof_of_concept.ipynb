{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook will be used to run different tests to identify what strategy would get us the best results in terms of imperceptible changes to audio that can poison music gen models. Each test will have either one or several experiments to determine the approach with the best results given each test's goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test1**:\n",
    "\n",
    "- The goal of this test is to determine whether the proof of concept can be reached if we work on modifying the spectrogram domain. Each experiment is a different implementation of perturbations in the spectogram domain. The goal is to try to find the implementation with the best results that allows us to add imperceptible perturbations in the spectrogram domain that can poison a music gen model\n",
    "\n",
    "**Test2**:\n",
    "- The goal of this test is to determine whether the proof of concept can be reached if we work on modifying the numpy representation of the audio inputs loaded with librosa. Each experiment is a different implementation of perturbations applied to the numpy representation. The goal is to try to find the implementation with the best results that allows us to add imperceptible perturbations applied to the numpy representation of a song that can poison a music gen model\n",
    "\n",
    "**Test3**: \n",
    "- Same but in the raw audio domain \n",
    "\n",
    "**Test4**: \n",
    "- Find a way to change audio based on features \n",
    "- extract features \n",
    "- reconstruct audio from features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these experiments will assume that there exists some fucked up vocals that were generated by RVC given the input audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note perturbed means slight changed; Changed means it's a different voice completely\n",
    "DATA_CHECKPOINT = '../data'\n",
    "INPUT_SONG_CHECKPOINT = 'original_wavs'\n",
    "CHANGED_SONG_CHECKPOINT = 'changed_wavs/mixes' #Songs that have vocals completely changed\n",
    "CHANGED_VOX_CHECKPOINT = 'changed_wavs/vocals' #Just vocals that are completely changed \n",
    "PERTURBED_SONG_CHECKPOINT = 'perturbed_wavs/mixes'\n",
    "PERTURBED_VOX_CHECKPOINT = 'perturbed_wavs/vocals'\n",
    "SPLIT_SONG_CHECKPONT = 'split_wavs'\n",
    "\n",
    "source_names = ['drums', 'bass', 'other', 'vocals']\n",
    "song_name = 'Westy - KING OF THE NIGHT'\n",
    "\n",
    "filename1 = f'Westy - KING OF THE NIGHT_{source_names[3]}' #Using the vocals split now \n",
    "filename2 = f'Westy - KING OF THE NIGHT_changed' #Using the completely changed vocals \n",
    "perturbed_output_filename = f'Westy - KING OF THE NIGHT_perturbed'\n",
    "file_format = '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed_wavs/vocals\n"
     ]
    }
   ],
   "source": [
    "audio_type = \"vocals\"\n",
    "folder = PERTURBED_SONG_CHECKPOINT if audio_type==\"song\" else PERTURBED_VOX_CHECKPOINT\n",
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(audio_type, test_number, experiment_number, perturbed_output_filename=perturbed_output_filename, file_format=file_format):\n",
    "    folder = PERTURBED_SONG_CHECKPOINT if audio_type==\"song\" else PERTURBED_VOX_CHECKPOINT\n",
    "    filename = perturbed_output_filename+f\"_test_{test_number}\"+f\"_exp{experiment_number}\"+file_format\n",
    "    return os.path.join(DATA_CHECKPOINT, folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_input_audio = os.path.join(DATA_CHECKPOINT, SPLIT_SONG_CHECKPONT, filename1+file_format)\n",
    "changed_input_audio = os.path.join(DATA_CHECKPOINT, CHANGED_VOX_CHECKPOINT, filename2+file_format) \n",
    "perturbed_vox_output_audio = os.path.join(DATA_CHECKPOINT, PERTURBED_VOX_CHECKPOINT, perturbed_output_filename+file_format)\n",
    "perturbed_mix_output_audio = os.path.join(DATA_CHECKPOINT, PERTURBED_SONG_CHECKPOINT, perturbed_output_filename+file_format)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(clean_input_audio)==True\n",
    "assert os.path.exists(changed_input_audio)==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test will focus on the **vocals** only. The input will be westy's vocals and the changed westy's vocals to ice spice. The output will be vocals that were created by placing some sort of \"ice spice\" envelope on the westy's vocals. The will be done on the spectrogram itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1**: change the entire audio based on a linear combination of the spectrograms of the input audio. The importance of each song is specified through the parameter overlay_weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) preprocess audio (normalize the input, truncate so they are exactly the same length)\n",
    "2) Extract and prepare audio features. This experiment will use the stft spectrogram spectrogram domain still \n",
    "3) Blend the changes in the spectrogram domain based on the overlay_weight parameter\n",
    "4) Convert the blended spectrogram back to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "\n",
    "# Configuration hyperparameters\n",
    "\n",
    "overlay_weight = 0.2  # weight for song2 overlay\n",
    "output_path = get_output_path(\"vocals\", test_number=\"1\", experiment_number=\"1\")\n",
    "\n",
    "print(\"[INFO] HYPERPARAMETERS:\\n\")\n",
    "print(f\"overlay_weight = {overlay_weight}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Load the audio files\n",
    "song1_path = clean_input_audio  # Path to the original song\n",
    "song2_path = changed_input_audio  # Path to the song with different vocals\n",
    "\n",
    "song1, sr = librosa.load(song1_path, sr=None)\n",
    "song2, _ = librosa.load(song2_path, sr=sr)  # Ensure same sample rate\n",
    "print(\"[INFO] INPUTS:\\n\")\n",
    "print(f\"Original audio found at {song1_path}\")\n",
    "print(f\"Corresponding changed audio found at {song2_path}\")\n",
    "print(f\"Sample Rate: {sr}\")\n",
    "print()\n",
    "\n",
    "# Ensure both songs are the same length\n",
    "min_length = min(len(song1), len(song2))\n",
    "song1 = song1[:min_length]\n",
    "song2 = song2[:min_length]\n",
    "\n",
    "# Step 2: Convert to spectrogram (frequency domain)\n",
    "def compute_spectrogram(audio, sr):\n",
    "    S = librosa.stft(audio, n_fft=2048, hop_length=512)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    return S, S_db\n",
    "\n",
    "print(\"[INFO] Generating spectrograms for each song...:\\n\")\n",
    "print(f\"Original audio found at {song1_path}\")\n",
    "print(f\"Corresponding changed audio found at {song2_path}\")\n",
    "print(f\"Sample Rate: {sr}\")\n",
    "\n",
    "# Compute spectrograms for both songs\n",
    "S1, S1_db = compute_spectrogram(song1, sr)\n",
    "S2, S2_db = compute_spectrogram(song2, sr)\n",
    "\n",
    "print(\"Spectrograms generated successfully!\\n\")\n",
    "\n",
    "def plot_spectrogram(S_db, title):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        librosa.display.specshow(S_db, sr=sr, hop_length=512, x_axis='time', y_axis='log')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "# Uncomment to visualize\n",
    "plot_spectrogram(S1_db, 'Original Song Spectrogram')\n",
    "plot_spectrogram(S2_db, 'Overlay Song Spectrogram')\n",
    "\n",
    "# Step 4: Blending in the frequency domain (spectrogram domain)\n",
    "# Create a mask that blends specific frequencies from song2\n",
    "blended_spectrogram = (1 - overlay_weight) * S1 + overlay_weight * S2\n",
    "\n",
    "# Step 5: Convert blended spectrogram back to time domain\n",
    "blended_audio = librosa.istft(blended_spectrogram, hop_length=512)\n",
    "\n",
    "# Step 6: Save the output\n",
    "sf.write(output_path, blended_audio, sr)\n",
    "print(f'Perturbed song saved to {output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test will focus on the **vocals** only. The input will be westy's vocals and the changed westy's vocals to ice spice. The output will be vocals that were created by placing some sort of \"ice spice\" envelope on the westy's vocals. This will be done by loading the song using librosa and converting it to a numpy array. We will segment the song based on specified segment_length and we will experiment with different sampling techniques when choosing the segments to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment1**: segment the audio input into segments of length segment_length, loop over every segment and change it based on a linear combination of both input audios for that segment. For a given segment the value of the segment will be computed as follows: new_segment = (1 - overlay_weight) x segment_in_song1 + overlay_weight x segment_in_song2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment2**: Instead of looping through all segments computing new segments based on a linear combination, we will randomly sample a percentage specified by the hyperparameter selected_segments_size from the total number of segments. We will then only modify the sampled segments and not all possible samples. if selected_segments_size is set to 1, we will get the output of the experiment1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment3**: Instead of changing all of the selected segment, we now only change the initial portion of the segment. The size of the segment is specified through the hyperparameter segment_pct. We will then only modify the selected portion of the segment. if segment_pct is set to 1, we will get the output of experiment 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) preprocess audio (normalize the input, truncate so they are exactly the same length)\n",
    "2) load the audio inputs using libroasa and change it into a numpy array\n",
    "3) Split the audio inputs into segments of length segment_length\n",
    "4) Loop over the selected segments and overlay the audio based on the overlay_weight:\n",
    "new_segment = (1 - overlay_weight) x segment_in_song1 + overlay_weight x segment_song2\n",
    "5) Convert the blended spectrogram back to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "\n",
    "def perturb_audio1(clean_audio_path, changed_audio_path, perturbed_audio_path, segment_length, overlay_weight):\n",
    "\n",
    "    print(\"[INFO] HYPERPARAMETERS:\\n\")\n",
    "    print(f\"segment_length = {segment_length}\")\n",
    "    print(f\"overlay_weight = {overlay_weight}\")\n",
    "    print()\n",
    "\n",
    "    # Step 1: Load the audio files\n",
    "    song1_path = clean_audio_path  # Path to the original song\n",
    "    song2_path = changed_audio_path  # Path to the song with different vocals\n",
    "\n",
    "    song1, sr = librosa.load(song1_path, sr=None)\n",
    "    song2, _ = librosa.load(song2_path, sr=sr)  # Ensure same sample rate\n",
    "    print(\"[INFO] INPUTS:\\n\")\n",
    "    print(f\"Original audio found at {song1_path}\")\n",
    "    print(f\"Corresponding changed audio found at {song2_path}\")\n",
    "    print(f\"Sample Rate: {sr}\")\n",
    "    print()\n",
    "\n",
    "    # Ensure both songs are the same length\n",
    "    min_length = min(len(song1), len(song2))\n",
    "    song1 = song1[:min_length]\n",
    "    song2 = song2[:min_length]\n",
    "\n",
    "\n",
    "    print(\"[INFO] Retrieving Number of Segments\\n\")\n",
    "    # Step 3: Divide song1 into segments and overlay song2 segments\n",
    "    num_segments = int(len(song1) / (segment_length * sr))\n",
    "    print(f\"SONG LENGTH: {len(song1)}\")\n",
    "    print(f\"NUMBER OF SEGMENTS OF {segment_length} FOUND: {num_segments}\")\n",
    "\n",
    "    # Initialize the blended song with a copy of song1\n",
    "    blended_audio = np.copy(song1)\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        # Define the time range for the current segment\n",
    "        start_sample = int(i * segment_length * sr)\n",
    "        end_sample = start_sample + int(segment_length * sr)\n",
    "        \n",
    "        # Segment in time domain\n",
    "        segment_song1 = song1[start_sample:end_sample]\n",
    "        segment_song2 = song2[start_sample:end_sample]\n",
    "        \n",
    "        # Blending in time domain\n",
    "        blended_segment = (1 - overlay_weight) * segment_song1 + overlay_weight * segment_song2\n",
    "        \n",
    "        # Place the blended segment back into the song\n",
    "        blended_audio[start_sample:end_sample] = blended_segment\n",
    "\n",
    "    # Save the output\n",
    "    sf.write(perturbed_audio_path, blended_audio, sr)\n",
    "    print(f'Perturbed song saved to {perturbed_audio_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1_output_path = get_output_path(\"vocals\", \"2\", \"1\")\n",
    "perturb_audio1(\n",
    "    clean_input_audio, \n",
    "    changed_input_audio, \n",
    "    experiment1_output_path, \n",
    "    segment_length=2.0, \n",
    "    overlay_weight=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) preprocess audio (normalize the input, truncate so they are exactly the same length)\n",
    "2) Split the original audio into segments of length segment_length (hyperparameter)\n",
    "3) use the hyperparameter selected_segments_size to determine the percentage of segments that will be changed. If the input has 1000 segments and percentage was set to 0.15, only 150 segments will be changed (chosen randomly)\n",
    "4) Loop over the selected segments and overlay the audio based on the overlay_weight:\n",
    "new_segment = (1 - overlay_weight) x segment_in_song1 + overlay_weight x segment_song2\n",
    "5) Convert the blended np array back to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def perturb_audio2(clean_audio_path, changed_audio_path, perturbed_audio_path, segment_length, overlay_weight, selected_segments_size):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    segment_length: number of seconds per segment \n",
    "    overlay_weight: weight for song2 overlay. Higher values will give more weight to the changed_audio_path \n",
    "    in the segments overlay step (audio more similar to the changed audio input)\n",
    "    selected_segments_size: percentage of segments that will be changed. If the audio has 1000 time segments and segments_pct\n",
    "    was set to 0.15, then only 150 time segments will be changed and overlayed. The remaining 850 time segments\n",
    "    will remain unchanged\n",
    "    \"\"\"\n",
    "    # Configuration hyperparameters\n",
    "    output_path = perturbed_audio_path\n",
    "\n",
    "    print(\"[INFO] HYPERPARAMETERS:\\n\")\n",
    "    print(f\"segment_length = {segment_length}\")\n",
    "    print(f\"overlay_weight = {overlay_weight}\")\n",
    "    print(f\"selected_segments_size = {selected_segments_size}\")\n",
    "    print()\n",
    "\n",
    "    # Step 1: Load the audio files\n",
    "    song1_path = clean_audio_path  # Path to the original song\n",
    "    song2_path = changed_audio_path  # Path to the song with different vocals\n",
    "\n",
    "    song1, sr = librosa.load(song1_path, sr=None)\n",
    "    song2, _ = librosa.load(song2_path, sr=sr)  # Ensure same sample rate\n",
    "    print(\"[INFO] INPUTS:\\n\")\n",
    "    print(f\"Original audio found at {song1_path}\")\n",
    "    print(f\"Corresponding changed audio found at {song2_path}\")\n",
    "    print(f\"Sample Rate: {sr}\")\n",
    "    print()\n",
    "\n",
    "    # Ensure both songs are the same length\n",
    "    min_length = min(len(song1), len(song2))\n",
    "    song1 = song1[:min_length]\n",
    "    song2 = song2[:min_length]\n",
    "\n",
    "    print(\"[INFO] Retrieving Number of Segments\\n\")\n",
    "    # Step 3: Divide song1 into segments and overlay song2 segments\n",
    "    num_segments = int(len(song1) / (segment_length * sr))\n",
    "    print(f\"SONG LENGTH: {len(song1)}\")\n",
    "    print(f\"NUMBER OF SEGMENTS OF {segment_length} COMPUTED: {num_segments}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize the blended song with a copy of song1\n",
    "    blended_audio = np.copy(song1)\n",
    "\n",
    "    print(\"[INFO] Sampling segments to perturb...\\n\")\n",
    "    selected_segments = random.sample(range(num_segments), int(num_segments * selected_segments_size))\n",
    "    print(f\"TOTAL TIME SEGMENTS: {num_segments}\")\n",
    "    print(f\"SEGMENTS SELECTED: {len(selected_segments)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    for i in selected_segments:\n",
    "        # Define the time range for the current segment\n",
    "        start_sample = int(i * segment_length * sr)\n",
    "        end_sample = start_sample + int(segment_length * sr)\n",
    "        \n",
    "        # Segment in time domain\n",
    "        segment_song1 = song1[start_sample:end_sample]\n",
    "        segment_song2 = song2[start_sample:end_sample]\n",
    "        \n",
    "        # Blending in time domain\n",
    "        blended_segment = (1 - overlay_weight) * segment_song1 + overlay_weight * segment_song2\n",
    "        \n",
    "        # Place the blended segment back into the song\n",
    "        blended_audio[start_sample:end_sample] = blended_segment\n",
    "\n",
    "    # Save the output\n",
    "    sf.write(output_path, blended_audio, sr)\n",
    "    print(f'Perturbed song saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] HYPERPARAMETERS:\n",
      "\n",
      "segment_length = 2.0\n",
      "overlay_weight = 0.2\n",
      "\n",
      "[INFO] INPUTS:\n",
      "\n",
      "Original audio found at ../data/split_wavs/Westy - KING OF THE NIGHT_vocals.wav\n",
      "Corresponding changed audio found at ../data/changed_wavs/vocals/Westy - KING OF THE NIGHT_changed.wav\n",
      "Sample Rate: 44100\n",
      "\n",
      "[INFO] Retrieving Number of Segments\n",
      "\n",
      "SONG LENGTH: 7990156\n",
      "NUMBER OF SEGMENTS OF 2.0 COMPUTED: 90\n",
      "\n",
      "[INFO] Sampling segments to perturb...\n",
      "\n",
      "TOTAL TIME SEGMENTS: 90\n",
      "SEGMENTS SELECTED: 13\n",
      "\n",
      "Perturbed song saved to ../data/perturbed_wavs/vocals/Westy - KING OF THE NIGHT_perturbed_exp2.wav\n"
     ]
    }
   ],
   "source": [
    "experiment2_output_path = get_output_path(\"vocals\", \"2\", \"2\")\n",
    "perturb_audio2(clean_input_audio, changed_input_audio, experiment2_output_path, 2.0, 0.2, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) preprocess audio (normalize the input, truncate so they are exactly the same length)\n",
    "2) Split the original audio into segments of length segment_length (hyperparameter)\n",
    "3) use the hyperparameter selected_segments_size to determine the percentage of segments that will be changed. If the input has 1000 segments and percentage was set to 0.15, only 150 segments will be changed (chosen randomly)\n",
    "4) Loop over the selected segments and select a portion of the segment to modify. The size of the portion is determined by the hyperparameter segment_pct. If segment_pct is set to 1, we will get the output of Experiment 2\n",
    "5) overlay the audio for the selected portion based on the overlay_weight:\n",
    "new_portion = (1 - overlay_weight) x portion_in_song1 + overlay_weight x portion_in_song1\n",
    "6) Convert the blended np array back to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def perturb_audio3(\n",
    "        clean_audio_path, \n",
    "        changed_audio_path, \n",
    "        perturbed_audio_path, \n",
    "        segment_length, \n",
    "        overlay_weight, \n",
    "        selected_segments_size,\n",
    "        segment_pct):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    segment_length: number of seconds per segment \n",
    "    overlay_weight: weight for song2 overlay. Higher values will give more weight to the changed_audio_path \n",
    "    in the segments overlay step (audio more similar to the changed audio input)\n",
    "    selected_segments_size: percentage of segments that will be changed. If the audio has 1000 time segments and segments_pct\n",
    "    was set to 0.15, then only 150 time segments will be changed and overlayed. The remaining 850 time segments\n",
    "    will remain unchanged\n",
    "    segment_pct: percentage of the segment that would be changed. For segment_length=2.0 and segment_pct=0.10, the first 150ms\n",
    "    of the segment will be chosen\n",
    "    \"\"\"\n",
    "    # Configuration hyperparameters\n",
    "    output_path = perturbed_audio_path\n",
    "\n",
    "    print(\"[INFO] HYPERPARAMETERS:\\n\")\n",
    "    print(f\"segment_length = {segment_length}\")\n",
    "    print(f\"overlay_weight = {overlay_weight}\")\n",
    "    print(f\"selected_segments_size = {selected_segments_size}\")\n",
    "    print(f\"segment_pct = {segment_pct}\")\n",
    "    print()\n",
    "\n",
    "    # Step 1: Load the audio files\n",
    "    song1_path = clean_audio_path  # Path to the original song\n",
    "    song2_path = changed_audio_path  # Path to the song with different vocals\n",
    "\n",
    "    song1, sr = librosa.load(song1_path, sr=None)\n",
    "    song2, _ = librosa.load(song2_path, sr=sr)  # Ensure same sample rate\n",
    "    print(\"[INFO] INPUTS:\\n\")\n",
    "    print(f\"Original audio found at {song1_path}\")\n",
    "    print(f\"Corresponding changed audio found at {song2_path}\")\n",
    "    print(f\"Sample Rate: {sr}\")\n",
    "    print()\n",
    "\n",
    "    # Ensure both songs are the same length\n",
    "    min_length = min(len(song1), len(song2))\n",
    "    song1 = song1[:min_length]\n",
    "    song2 = song2[:min_length]\n",
    "\n",
    "    # Step 2: Convert to spectrogram (frequency domain)\n",
    "    def compute_spectrogram(audio, sr):\n",
    "        S = librosa.stft(audio, n_fft=2048, hop_length=512)\n",
    "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "        return S, S_db\n",
    "\n",
    "    print(\"[INFO] Retrieving Number of Segments\\n\")\n",
    "    # Step 3: Divide song1 into segments and overlay song2 segments\n",
    "    num_segments = int(len(song1) / (segment_length * sr))\n",
    "    print(f\"SONG LENGTH: {len(song1)}\")\n",
    "    print(f\"NUMBER OF SEGMENTS OF {segment_length} COMPUTED: {num_segments}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize the blended song with a copy of song1\n",
    "    blended_audio = np.copy(song1)\n",
    "\n",
    "    print(\"[INFO] Sampling segments to perturb...\\n\")\n",
    "    selected_segments = random.sample(range(num_segments), int(num_segments * selected_segments_size))\n",
    "    print(f\"TOTAL TIME SEGMENTS: {num_segments}\")\n",
    "    print(f\"SEGMENTS SELECTED: {len(selected_segments)}\")\n",
    "    print()\n",
    "\n",
    "    for i in (selected_segments):\n",
    "        # Define the time range for the current segment\n",
    "        start_sample = int(i * segment_length * sr)\n",
    "        end_sample = start_sample + int(segment_length * sr)\n",
    "        \n",
    "        \n",
    "        # Define the portion of the segment to modify based on segment_pct\n",
    "        portion_length = int(segment_pct * segment_length * sr)\n",
    "        end_portion = start_sample + portion_length\n",
    "\n",
    "        # Segment in time domain\n",
    "        segment_song1 = song1[start_sample:end_portion]\n",
    "        segment_song2 = song2[start_sample:end_portion]\n",
    "\n",
    "        # Blending in time domain\n",
    "        blended_segment = (1 - overlay_weight) * segment_song1 + overlay_weight * segment_song2\n",
    "        \n",
    "        # Place the blended segment back into the song\n",
    "        blended_audio[start_sample:end_portion] = blended_segment\n",
    "\n",
    "    #Save the output\n",
    "    sf.write(output_path, blended_audio, sr)\n",
    "    print(f'Perturbed song saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] HYPERPARAMETERS:\n",
      "\n",
      "segment_length = 2.0\n",
      "overlay_weight = 0.2\n",
      "\n",
      "[INFO] INPUTS:\n",
      "\n",
      "Original audio found at ../data/split_wavs/Westy - KING OF THE NIGHT_vocals.wav\n",
      "Corresponding changed audio found at ../data/changed_wavs/vocals/Westy - KING OF THE NIGHT_changed.wav\n",
      "Sample Rate: 44100\n",
      "\n",
      "[INFO] Retrieving Number of Segments\n",
      "\n",
      "SONG LENGTH: 7990156\n",
      "NUMBER OF SEGMENTS OF 2.0 COMPUTED: 90\n",
      "\n",
      "[INFO] Sampling segments to perturb...\n",
      "\n",
      "TOTAL TIME SEGMENTS: 90\n",
      "SEGMENTS SELECTED: 13\n",
      "\n",
      "Perturbed song saved to ../data/perturbed_wavs/vocals/Westy - KING OF THE NIGHT_perturbed_exp3.wav\n"
     ]
    }
   ],
   "source": [
    "experiment3_output_path = get_output_path(\"vocals\", \"2\", \"3\")\n",
    "perturb_audio3(clean_input_audio, changed_input_audio, experiment3_output_path, segment_length=2.0, overlay_weight=0.2, selected_segments_size=0.15,segment_pct=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
